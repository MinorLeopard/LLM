# -*- coding: utf-8 -*-
"""dataset_reliance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1erl6ZqjRIWnPYQOQQgxYA-5FSRomHt3V
"""

import pandas as pd
import json

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define the base path for saving and loading files
base_path = '/content/drive/MyDrive/datasets/'

# Load datasets
ohlcv_data = pd.read_csv(base_path + 'ohlcv_data_NSE_EQ_INE002A01018_2024-05-30.csv')
hardy_indicator_data = pd.read_csv(base_path + 'hardy_indicator_data_NSE_EQ_INE002A01018_2024-05-30.csv')
obv_data = pd.read_csv(base_path + 'obv_data_NSE_EQ_INE002A01018_2024-05-30.csv')
rsi_histo_alert_data = pd.read_csv(base_path + 'rsiHistoAlert_data_NSE_EQ_INE002A01018_2024-05-30.csv')
trendlines_data = pd.read_csv(base_path + 'trendlines_data_NSE_EQ_INE002A01018_2024-05-30.csv')
volume_spike_data = pd.read_csv(base_path + 'volume_spike_data_NSE_EQ_INE002A01018_2024-05-30.csv')

# Merge datasets on common keys (e.g., 'Date')
merged_data = ohlcv_data.merge(hardy_indicator_data, on=['Date'], how='left',suffixes=('', '_hardy'))
merged_data = merged_data.merge(obv_data, on=['Date'], how='left',suffixes=('', '_obv'))
merged_data = merged_data.merge(rsi_histo_alert_data, on=['Date'], how='left',suffixes=('', '_rsi'))
merged_data = merged_data.merge(trendlines_data, on=['Date'], how='left',suffixes=('', '_trendlines'))
merged_data = merged_data.merge(volume_spike_data, on=['Date'], how='left',suffixes=('', '_volumespike'))

# Generate context-question-answer pairs
def generate_qa_pairs(row):
    context = (f"Date: {row['Date']}, Open: {row['Open']}, High: {row['High']}, "
               f"Low: {row['Low']}, Close: {row['Close']}, Volume: {row['Volume']}, "
               f"SMA: {row.get('SMA', 'N/A')}, EMA: {row.get('EMA', 'N/A')}, "
               f"RSI: {row.get('RSI', 'N/A')}, Signal: {row.get('Signal', 'N/A')}, "
               f"Volume Spike: {row.get('Volume Spike', 'N/A')}, OBV: {row.get('OBV', 'N/A')}, "
               f"Hardy Indicator: {row.get('Hardy Indicator', 'N/A')}")

    questions = [
        f"What is the live price of Reliance?",
        f"Tell me the closing price of Reliance on {row['Date']}.",
        f"What are the moving averages for Reliance?",
        f"What is the RSI for Reliance?",
        f"What signals are there for Reliance?",
        f"What were good buying positions for Reliance?",
        f"When were there volume spikes for Reliance?",
        f"What is the highest price of Reliance in the past year?",
        f"What is the lowest price of Reliance in the past year?",
        f"What is the average closing price of Reliance over the past month?",
        f"Are there any buy signals for Reliance?",
        f"Are there any sell signals for Reliance?",
        f"What are the recent volume spikes for Reliance?",
        f"What are the latest trendlines for Reliance?",
        f"Is Reliance a good investment?",
        f"What are the recent news updates about Reliance?",
        f"What are the financial ratios of Reliance?",
        f"What is the market cap of Reliance?",
        f"What is the dividend yield of Reliance?"
    ]

    answers = [
        f"The live price of Reliance is {row['Close']}.",
        f"The closing price of Reliance on {row['Date']} was {row['Close']}.",
        f"The moving averages for Reliance are SMA: {row.get('SMA', 'N/A')}, EMA: {row.get('EMA', 'N/A')}.",
        f"The RSI for Reliance is {row.get('RSI', 'N/A')}.",
        f"The signals for Reliance are {row.get('Signal', 'N/A')}.",
        f"Good buying positions for Reliance were on the dates with buy signals: {row.get('Buy Signal Dates', 'N/A')}.",
        f"There were volume spikes for Reliance on these dates: {row.get('Volume Spike Dates', 'N/A')}.",
        f"The highest price of Reliance in the past year was {row['High']}.",
        f"The lowest price of Reliance in the past year was {row['Low']}.",
        f"The average closing price of Reliance over the past month was {row['Close']}.",
        f"Yes, there are buy signals for Reliance.",
        f"Yes, there are sell signals for Reliance.",
        f"The recent volume spikes for Reliance occurred on these dates: {row.get('Volume Spike Dates', 'N/A')}.",
        f"The latest trendlines for Reliance indicate the following trends: {row.get('Trendlines', 'N/A')}.",
        f"Investing in Reliance could be considered based on the following factors: {row.get('Investment Factors', 'N/A')}.",
        f"The recent news updates about Reliance are: {row.get('News Updates', 'N/A')}.",
        f"The financial ratios of Reliance are: {row.get('Financial Ratios', 'N/A')}.",
        f"The market cap of Reliance is {row.get('Market Cap', 'N/A')}.",
        f"The dividend yield of Reliance is {row.get('Dividend Yield', 'N/A')}."
    ]

    qa_pairs = []
    for question, answer in zip(questions, answers):
        qa_pairs.append({
            "context": context,
            "question": question,
            "answer": answer
        })

    return qa_pairs

# Generate QA pairs for the entire dataset
qa_dataset = []
for _, row in merged_data.iterrows():
    qa_dataset.extend(generate_qa_pairs(row))

# Save the dataset to a JSON file in Google Drive
with open(base_path + 'qa_RELIANCE_dataset.json', 'w') as f:
    json.dump(qa_dataset, f, indent=4)

# Display a sample of the generated QA pairs
print(json.dumps(qa_dataset[:5], indent=4))

"""Import Datasets"""

!pip install datasets

"""Utilizing QLoRA for better training speeds"""

!pip install transformers datasets peft bitsandbytes

"""Load custom dataset and preprocess it."""

import pandas as pd
import json
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig
from datasets import Dataset, DatasetDict
from peft import LoraConfig, get_peft_model
import bitsandbytes as bnb
import torch

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the custom dataset
with open('/content/drive/MyDrive/datasets/qa_RELIANCE_dataset.json', 'r') as f:
    qa_dataset = json.load(f)

# Convert the list of dictionaries to a dictionary of lists
def list_to_dict(data):
    result = {}
    for key in data[0].keys():
        result[key] = [d[key] for d in data]
    return result

qa_dict = list_to_dict(qa_dataset)

# Convert to a Hugging Face Dataset
dataset = Dataset.from_dict(qa_dict)

# Split the dataset into train and validation sets
train_test_split = dataset.train_test_split(test_size=0.1)
datasets = DatasetDict({
    'train': train_test_split['train'],
    'test': train_test_split['test']
})

# Load the tokenizer and model with 8-bit quantization and CPU offloading
model_id = "meta-llama/Meta-Llama-3-8B"  # Replace with the actual model name
tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)



model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto"  # Allow transformers to automatically map model parts to devices
)

"""Tokenize the dataset"""

# Add padding token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

tokenizer.pad_token = tokenizer.pad_token or '[PAD]'


# Tokenize the dataset with fixed max_length
def tokenize_function(example):
    question = " ".join(example['question']) if isinstance(example['question'], list) else str(example['question'])
    context = " ".join(example['context']) if isinstance(example['context'], list) else str(example['context'])
    tokens = tokenizer(question + " " + context, truncation=True, padding='max_length', max_length=512)
    return tokens

# Apply the tokenization and length enforcement
tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=datasets["train"].column_names)

# Debug: Check the tokenized datasets
print(f"Tokenized train dataset size: {len(tokenized_datasets['train'])}")
print(f"Tokenized test dataset size: {len(tokenized_datasets['test'])}")

# Debug: Print a few examples to verify correctness
for i in range(3):
    print(f"Train Example {i}: {tokenized_datasets['train'][i]}")
    print(f"Test Example {i}: {tokenized_datasets['test'][i]}")

# Remove unused columns and set format
tokenized_datasets.set_format("torch")

# Debug: Print the shapes of a few examples
for i in range(3):
    print(f"Example {i} input_ids length: {len(tokenized_datasets['train'][i]['input_ids'])}")
    print(f"Example {i} attention_mask length: {len(tokenized_datasets['train'][i]['attention_mask'])}")

"""Configure QLoRA for efficient fine-tuning."""

from peft import LoraConfig, get_peft_model

# Define Lora configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Target specific modules for LoRA
    lora_dropout=0.1,
    bias="none"
)

# Apply Lora configuration to the model
model = get_peft_model(model, lora_config)

"""Finetuning Llama3 based on the dataset created"""

# Define training arguments with reduced epochs
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,  # Reduced number of epochs
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_steps=10,
    eval_strategy="epoch",  # Updated according to the deprecation warning
    save_total_limit=2,
    fp16=True,
    gradient_accumulation_steps=4,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

# Train the model
trainer.train()

# Save the model to Google Drive
model.save_pretrained('/content/drive/MyDrive/finetuned_llama3')
tokenizer.save_pretrained('/content/drive/MyDrive/finetuned_llama3')

import pandas as pd
import json
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset
from huggingface_hub import login

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the custom dataset
with open('/content/drive/MyDrive/datasets/qa_RELIANCE_dataset.json', 'r') as f:
    qa_dataset = json.load(f)

# Convert the list of dictionaries to a dictionary of lists
def list_to_dict(data):
    result = {}
    for key in data[0].keys():
        result[key] = [d[key] for d in data]
    return result

qa_dict = list_to_dict(qa_dataset)

# Convert to a Hugging Face Dataset
dataset = Dataset.from_dict(qa_dict)


model_id = "meta-llama/Meta-Llama-3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Tokenize the dataset
def tokenize_function(example):
    return tokenizer(example['question'] + " " + example['context'], truncation=True, padding='max_length', max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["context", "question", "answer"])
tokenized_dataset.set_format("torch")

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_steps=10,
    evaluation_strategy="epoch",
    save_total_limit=2,
    fp16=True,
    gradient_accumulation_steps=4,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

# Train the model
trainer.train()

# Save the model to Google Drive
model.save_pretrained('/content/drive/MyDrive/finetuned_llama3')
tokenizer.save_pretrained('/content/drive/MyDrive/finetuned_llama3')